

# åŸºæœ¬åº“ -- å‘æœåŠ¡å™¨å‘å‡ºè¯·æ±‚

- æœ€åˆçš„çˆ¬è™«æ“ä½œå°±æ˜¯æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘å‡ºè¯·æ±‚
  - æ„é€ è¯·æ±‚
  - æ•°æ®ç»“æ„çš„å®ç°
  - HTTP/TCP/IP å±‚çš„ç½‘ç»œå®ç°
- Python æä¾›äº† `urllib`, `httplib2`, `requests` å’Œ `treq` ç”¨äºå‘é€è¯·æ±‚ï¼Œå¹¶ä¸”æ©ç›–äº†ä¸Šè¿°ç§ç§çš„å®ç°ç»†èŠ‚ã€‚

## `urllib`

### å†å²

- Python 2 ä¸­ï¼Œæœ‰ urllib å’Œ urllib2 ä¸¤ä¸ªåº“æä¾›è¯·æ±‚çš„å‘é€
- Python 3 ä¸­ï¼Œå·²ç»ä¸å­˜åœ¨ urllib2 è¿™ä¸ªåº“ï¼Œå¹¶ä¸”ç»Ÿä¸€ä¸º urllib
- æ–‡æ¡£åœ¨ <https://docs.python.org/3/library/urllib.html>

### æ¨¡å—

- `request` åŸºæœ¬çš„ `HTTP` è¯·æ±‚æ¨¡å—ï¼Œç”¨äºæ¨¡æ‹Ÿå‘é€è¯·æ±‚
- `error` å¼‚å¸¸å¤„ç†
- `parse` è§£æ `url` å¹¶åˆå¹¶ï¼Œæ‹†åˆ†ï¼Œæˆ–è¿›è¡Œæ“ä½œ
- `robotparser` åˆ¤æ–­å¯ä»¥çˆ¬å–çš„ç½‘ç«™ã€‚ä½¿ç”¨å¾ˆå°‘ï¼ˆéƒ½ä¸Šçˆ¬è™«äº†è¿˜å…³å¿ƒè¿™ä¸ªï¼ŸğŸ™‚ï¼‰

### `request`

- å®šä¹‰äº†é€‚ç”¨äºåœ¨å„ç§å¤æ‚æƒ…å†µä¸‹æ‰“å¼€ URLï¼ˆä¸»è¦ä¸º HTTPï¼‰çš„å‡½æ•°å’Œç±» -- ä¾‹å¦‚åŸºæœ¬è®¤è¯ã€æ‘˜è¦è®¤è¯ã€é‡å®šå‘ã€cookies ç­‰

#### `urlopen`

- `urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)`

- çˆ¬å– `Python` å®˜ç½‘
  - ```python
    from urllib.request import urlopen
    
    response = urlopen("https://www.python.org")
    print(response.read().decode("utf-8"))
    ```

- è¿™ä¸ªæ–¹æ³•è¿”å›ä¸€ä¸ª `http.client.HTTPResponse` ç±»å‹çš„å¯¹è±¡ã€‚

  - `read()`,  `readinto()` å¾—åˆ°è¿”å›çš„ç½‘é¡µå†…å®¹

  - `getheader(name)`, `getheaders()`

  - `fileno()` ç­‰æ–¹æ³•

  - `msg`, `version`, `status`ï¼ˆè¿”å›ç»“æœçš„çŠ¶æ€ç ï¼‰, `reason`, `debuglevel`, å’Œ `closed` ç­‰å±æ€§

  - å°è¯•è§£æ `HTTPResponse` å¯¹è±¡ä¸­çš„æœ‰ç”¨ä¿¡æ¯

    - ```python
      from urllib.request import urlopen
      
      response = urlopen("https://www.python.org")
      print(response.status)  # 200, OK
      print(response.getheaders())  # List of tuple
      print(response.getheader("Server"))  # nginx
      print(response.getheader("DoesNotExist"))  # None
      ```

- æ›´å¤šå‚æ•°çš„åˆ©ç”¨

  - `data`

    - é»˜è®¤çš„è¯·æ±‚æ–¹å¼æ˜¯ `GET`ã€‚å¦‚æœæƒ³è¦ä¼ é€’æ•°æ®ï¼Œå°±éœ€è¦ä½¿ç”¨ `POST` æ–¹å¼ã€‚

    - æŠŠæ•°æ®ç¼–ç æˆå­—èŠ‚æµç±»å‹ã€‚è½¬æ¢ä¹‹å‰ï¼Œä½¿ç”¨ `urlencode` æŠŠå‚æ•°å­—å…¸è½¬åŒ–ä¸ºå­—ç¬¦ä¸²ã€‚

    - ä½¿ç”¨ `httpbin.org` æ¥è¿›è¡Œ `HTTP` è¯·æ±‚æµ‹è¯•ï¼Œå¯ä»¥æµ‹è¯• `POST` è¯·æ±‚å¹¶è¾“å‡ºè¯·æ±‚çš„ç›¸å…³ä¿¡æ¯ã€‚

    - ```python
      from urllib import parse, request
      
      data = bytes(parse.urlencode({"word": "hello-world"}), encoding="utf-8")
      response = request.urlopen("http://httpbin.org/post", data=data)
      print(response.read().decode("utf-8"))
      ```

    - ```json
      {
        "args": {}, 
        "data": "", 
        "files": {}, 
        "form": {
          "word": "hello-world"
        }, 
        "headers": {
          "Accept-Encoding": "identity", 
          "Content-Length": "16", 
          "Content-Type": "application/x-www-form-urlencoded", 
          "Host": "httpbin.org", 
          "User-Agent": "Python-urllib/3.10", 
          "X-Amzn-Trace-Id": "Root=1-628d7e85-19608fec285061207e6171c8"
        }, 
        "json": null, 
        "origin": "24.131.62.201", 
        "url": "http://httpbin.org/post"
      }
      ```

  - `timeout`

    - è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œå•ä½ä¸ºç§’ã€‚å¦‚æœè¯·æ±‚è¶…å‡ºäº†è®¾ç½®çš„æ—¶é—´ï¼Œå¹¶ä¸”æ²¡æœ‰å¾—åˆ°å“åº”ï¼Œå°±ä¼šæŠ›å‡ºå¼‚å¸¸ã€‚å¦åˆ™ï¼Œå°±ä¼šä½¿ç”¨å…¨å±€é»˜è®¤æ—¶é—´ã€‚

    - ```python
      import socket
      from urllib import request, error
      
      try:
          response = request.urlopen("http://httpbin.org/get", timeout=0.001)
      except error.URLError as e:
          if isinstance(e.reason, socket.timeout):
              print("TIME OUT")
      ```

  - å…¶ä»–å‚æ•°

    - `context`ï¼Œå¿…é¡»æ˜¯ `ssl.SSLContext` ç±»å‹ï¼Œç”¨äºæŒ‡å®š `SSL` è®¾ç½®
    - `cafile` å’Œ `capath` æŒ‡å®š CA è¯ä¹¦ï¼Œå¯¹äº HTTPS é“¾æ¥ç”¨å¤„å¾ˆå¤§

#### `Request`

- ä¸ºäº†å®Œå–„è¯·æ±‚ï¼Œä¾‹å¦‚åŠ å…¥ `Headers` ç­‰çš„ä¿¡æ¯ï¼Œåº”å½“ä½¿ç”¨ `Request` ç±»æ¥å®ç°ã€‚

- `Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)`
  - `url` å°±æ˜¯ URLï¼Œæ˜¯å¿…é¡»ä¼ å…¥çš„
  - `data` åªæ¥å—å­—èŠ‚æµç±»å‹ã€‚å¯ä»¥ä½¿ç”¨ `urllib.parse.urlencode()` è¿›è¡Œç¼–ç ã€‚
  - `headers` å¯ä»¥åœ¨æ„é€ æ–¹æ³•ä¸­æŒ‡å®šï¼Œæˆ–è€… `add_header()` çš„å®ä¾‹æ–¹æ³•çš„æŒ‡å®šã€‚å¸¸ç”¨çš„æ˜¯ä½¿ç”¨ `user-Agent` æ¥ä¼ªè£…ï¼Œé»˜è®¤ä¸º `Python-urllib/version`
  - `origin_req_host` è¯·æ±‚æ–¹çš„ `IP` åœ°å€
  - `unverifiable` è¡¨ç¤ºè¯·æ±‚æ— æ³•éªŒè¯ã€‚è¿™æ ·çš„è¯·æ±‚ï¼Œè¡¨ç¤ºç”¨æˆ·å¯èƒ½æ²¡æœ‰è¶³å¤Ÿçš„æƒé™æ¥æ¥å—è¯·æ±‚çš„ç»“æœã€‚
  - `method` æ¥å—å­—ç¬¦ä¸²ï¼Œå¦‚ `GET`, `POST`, `PUT`, `DELETE` ç­‰ã€‚è§ [[Internet Basic, HTTP(s), TCP, and UDP]]

- ä¾‹å­

  - ```python
    from urllib import request, parse
    
    url = "http://httpbin.org/post"
    headers = {
        "User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:100.0) Gecko/20100101 Firefox/100.0",
        "Accept-Language": "zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2",
        "Host": "httpbin.org",
    }
    
    data = {"name": "Germany"}
    data = bytes(parse.urlencode(data), "utf-8")
    
    req = request.Request(url, data, headers, method="POST")
    response = request.urlopen(req)
    print(response.read().decode("utf-8"))
    ```

  - è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¯ä»¥æ›´åŠ æ–¹ä¾¿çš„æ„é€ è¯·æ±‚ï¼Œå¹¶å®ç°è¯·æ±‚çš„å‘é€äº†ã€‚

#### Handler

- `BaseHandler` æ˜¯ `urllib.request` æ¨¡å—ä¸­å…¶ä»–æ‰€æœ‰ `Handler` çš„çˆ¶ç±»ã€‚æä¾›äº†åŸºæœ¬çš„æ–¹æ³•ï¼Œä¾‹å¦‚ `default_open`, `protocol_request` ç­‰æ‰€æœ‰å­ç±»éƒ½éœ€è¦å®ç°çš„åŸºæœ¬æ–¹æ³•ã€‚
  - `HTTPDefaultErrorHandler` å¤„ç† HTTP å“åº”é”™è¯¯
  - `HTTPRedirectHandler` å¤„ç†é‡å®šå‘
  - `HTTPCookieProcesor` å¤„ç† Cookies
  - `ProxyHandler` ç®¡ç†ä»£ç†
  - `HTTPPasswordMgr` ç®¡ç†å¯†ç ï¼Œç»´æŠ¤ç”¨æˆ·åå’Œå¯†ç çš„è¡¨æ ¼
  - `HTTPBasicAuthHandler` è§£å†³è®¤è¯çš„é—®é¢˜ã€‚

#### Opener

- é€šè¿‡ä½¿ç”¨ `Handler` ï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºä¸€ä¸ª`Opener`

##### å¤„ç†éªŒè¯

```python
from urllib.request import (
    HTTPPasswordMgrWithDefaultRealm,
    HTTPBasicAuthHandler,
    build_opener,
)
from urllib.error import URLError

user = "username"
passwd = "password"
url = "http://localhost:5000"

p = HTTPPasswordMgrWithDefaultRealm()
p.add_password(None, url, user, passwd)
auth = HTTPBasicAuthHandler(p)
opener = build_opener(auth)

try:
    result = opener.open(url)
    html = result.read().decode("utf-8")
    print(html)
except URLError as e:
    print(f"Failed, because {e.reason!s}")
```

- å®ä¾‹åŒ– `HTTPBasicAuthHandler`ã€‚è¯¥å¯¹è±¡æ¥å—ä¸€ä¸ª `HTTPPasswordMgrWithDefaultRealm` å¯¹è±¡ï¼Œè¯¥é˜Ÿå‘å­˜å‚¨ç”¨æˆ·åå’Œå¯†ç ã€‚æœ€ç»ˆï¼Œå¾—åˆ°ä¸€ä¸ªå¤„ç†éªŒè¯çš„ `Handler`
- è¿™ä¸ª `Handler` ç”¨äºæ„å»º `Opener` ä»¥æ‰“å¼€é“¾æ¥

##### ä»£ç†

```python
from urllib.error import URLError
from urllib.request import ProxyHandler, build_opener

proxy = ProxyHandler(
    {"http": "http://127.0.0.1:9743", "https": "https://127.0.0.1:9743"}
)
opener = build_opener(proxy)
try:
    response = opener.open("https://www.google.com")
    print(response.read().decode("utf-8"))
except URLError as e:
    print(e.reason)
```

- ä¸Šè¿°çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬åœ¨æœ¬åœ°æ­å»ºä¸€ä¸ªä»£ç†ï¼Œä½¿å…¶è¿è¡Œåœ¨ 9743 ç«¯å£ä¸Šã€‚
- `ProxyHandler` æ¥å—ä¸€ä¸ªå­—å…¸ä½œæ–‡å‚æ•°ï¼Œé”®ä¸ºåè®®ç±»å‹ï¼Œå€¼ä¸ºä»£ç†é“¾æ¥ï¼Œå¯ä»¥æ·»åŠ éƒ½å„ä»£ç†ã€‚
- ç„¶åï¼Œæ„é€  `Opener` å¹¶ä½¿ç”¨å³å¯ã€‚

##### Cookies

###### è·å¾—

```python
from http import cookiejar
from urllib import request
from urllib.error import URLError

cookies = cookiejar.CookieJar()
handler = request.HTTPCookieProcessor(cookies)
opener = request.build_opener(handler)
try:
    response = opener.open("https://www.google.com")
    for item in cookies:
        print(f"{item.name:<10} = {item.value}")
except URLError as e:
    print(e.reason)
```

- å£°æ˜ `CookieJar` å¯¹è±¡ï¼Œç„¶ååˆ©ç”¨ `HTTPCookieProcessor` æ¥æ„å»ºä¸€ä¸ª `Handler`ã€‚ æœ€åï¼Œä½¿ç”¨ `build_opener` æ„å»º `opener` å³å¯ã€‚

###### ä¿å­˜

- MozillaCookieJar

  - ```python
    from distutils.command.build import build
    from http.cookiejar import MozillaCookieJar
    from urllib.error import URLError
    from urllib.request import HTTPCookieProcessor, build_opener
    
    filename = "cookies.txt"
    cookie = MozillaCookieJar(filename)
    opener = build_opener(HTTPCookieProcessor(cookie))
    try:
        response = opener.open("https://www.google.com")
        cookie.save(ignore_discard=True, ignore_expires=True)
    except URLError as e:
        print(e.reason)
    ```

  - ä¿å­˜å‡ºæ¥çš„ `MozillaCookieJar` å¤§è‡´ä¸ºï¼š

    - ```txt
      # Netscape HTTP Cookie File
      # http://curl.haxx.se/rfc/cookie_spec.html
      # This is a generated file!  Do not edit.
      
      .google.com	TRUE	/	TRUE	1656070883	1P_JAR	2022-05-25-11
      .google.com	TRUE	/	TRUE	1669030883	AEC	AakniGOEcEuk63Wk_JB2I0O6eXGL98xufeV1LQ2dL3fgRyYVFYmsK70izfw
      .google.com	TRUE	/	FALSE	1669290083	NID	511=fpqe6Qab1XUxrD4zF0QmEi_qXorwhGszsHZSF-JXm1naTm2cHl5rALvs5rhdiKF3gWR36MUmuDKxUkF1_M9pxOBkSfMSnsp9LdUGeChiYlawt5lGnrtSAn_VYCmCiKTDAnQdWrCg5XB0xzu3KAeBVcBz6yuzAO-ga_SNvKxQkoQ
      ```

- LWPCookieJar

  - åªéœ€è¦æŠŠ `MozzilaCookieJar` æ›¿æ¢ä¸º `LWPCOokieJar` å³å¯ã€‚ç”Ÿæˆçš„å†…å®¹å¦‚ä¸‹ï¼š

  - ```txt
    #LWP-Cookies-2.0
    Set-Cookie3: 1P_JAR="2022-05-25-11"; path="/"; domain=".google.com"; path_spec; domain_dot; secure; expires="2022-06-24 11:43:57Z"; version=0
    Set-Cookie3: AEC="AakniGMANGYLqj-B0q9LedkbSVLS3m88a9SjVja-fRHdIdD3eAqlxLmN6A"; path="/"; domain=".google.com"; path_spec; domain_dot; secure; expires="2022-11-21 11:43:57Z"; HttpOnly=None; SameSite=lax; version=0
    Set-Cookie3: NID="511=pV2VlkNkERQ0_5bJ6VBOZYkuLixPIVGyNb6GxQqsqlbj1RIICICgjtcmH6AST_R40AY-SFWTr27G3J5vcPsxS0zXouS1y0zwVnybuB6wlAkbbOo6tweXV-raWPyC7w0PSgF2wPlZCkuZerRL2fGmIBNbdWynhZetAFntVZvhb3c"; path="/"; domain=".google.com"; path_spec; domain_dot; expires="2022-11-24 11:43:57Z"; HttpOnly=None; version=0
    ```

###### åŠ è½½

```python
from http.cookiejar import LWPCookieJar
from urllib.error import URLError
from urllib.request import HTTPCookieProcessor, build_opener

cookie = LWPCookieJar()
cookie.load("cookies.txt", ignore_discard=True, ignore_expires=True)
handler = HTTPCookieProcessor(cookie)
opener = build_opener(handler)
try:
    response = opener.open("https://www.google.com")
    print(response.read().decode("utf-8"))
except URLError as e:
    print(e.reason)
```

### `error`

- `error` æ¨¡å—å®šä¹‰äº†è¿™ä¸ªæ¨¡å—å¯èƒ½äº§ç”Ÿçš„å¼‚å¸¸ã€‚å¦‚æœå‡ºç°é—®é¢˜ï¼Œ`request` æ¨¡å—å°±ä¼šæŠ›å‡º `error` ä¸­çš„å¼‚å¸¸ã€‚

#### `URLError`

- `URLError` æ˜¯å¼‚å¸¸æ¨¡å—çš„åŸºç±»ï¼Œç»§æ‰¿ä¸ `OSError`

  - æœ‰ä¸€ä¸ªå±æ€§ `reason`ï¼Œå¯ä»¥è¿”å›å¼‚å¸¸é”™è¯¯çš„åŸå› ã€‚

  - ä¾‹å¦‚ï¼Œæ‰“å¼€ä¸€ä¸ªä¸å­˜åœ¨çš„é¡µé¢ï¼Œé€šè¿‡æ•è· `URLError` å¯ä»¥è¿›è¡Œå¼‚å¸¸å¤„ç†ã€‚

  - ```python
    from urllib.error import URLError
    from urllib.request import urlopen
    
    try:
        response = urlopen("https://www.doesnotexistsreallyitshouldnot.com")
    except URLError as e:
        print(e.reason)
    ```

#### `HTTPError`

- `HTTPError` ç»§æ‰¿ä¸ `URLError`ï¼Œä¸“é—¨å¤„ç† `HTTP` è¯·æ±‚çš„å¼‚å¸¸ã€‚

  - `code` è¿”å› `HTTP` çŠ¶æ€ç 

  - `reason` åŒçˆ¶ç±»

  - `headers` è¿”å›è¯·æ±‚å¤´

  - ```python
    from urllib import request, error
    
    try:
        response = request.urlopen("https://www.cuiqingcai.com/index.htm")
    except error.HTTPError as e:
        print(e.reason, e.code, e.headers, seq="\n--------\n")
    ```

- æ­¤å¤„æ•è·çš„å¼‚å¸¸ï¼Œè¾“å‡ºåŸå› ã€çŠ¶æ€ç ã€ä»¥åŠè¯·æ±‚å¤´ã€‚

- æ›´å¥½çš„æ–¹å¼å¯ä»¥å…ˆæ•è·å­ç±»çš„é”™è¯¯ï¼Œç„¶åå†å¤„ç†çˆ¶ç±»çš„é”™è¯¯ã€‚

  - ```python
    from urllib import request, error
    
    try:
        response = request.urlopen("https://www.cuiqingcai.com/index.htm")
    except error.HTTPError as e:
        print(e.reason, e.code, e.headers, seq="\n--------\n")
    except error.URLError as e:
        print(e.reason)
    else:
        print("Success")
    ```

  - æœ‰æ—¶å€™ï¼Œ`reason` è¿”å›çš„å¯èƒ½æ˜¯ä¸€ä¸ªå¯¹è±¡ã€‚ä¾‹å¦‚ï¼Œ`socket.timeout` å¯¹è±¡ç­‰ã€‚æ­¤æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ `isinstance` çš„æ–¹å¼æ¥åˆ¤æ–­å…¶ç±»å‹ï¼Œåšå‡ºæ›´è¯¦ç»†çš„å¼‚å¸¸åˆ¤æ–­ã€‚

### `parse`

- æä¾›äº†å¤„ç† `URL` çš„æ ‡å‡†æ¥å£ï¼Œä¾‹å¦‚å®ç°å¯¹ `URL` å„éƒ¨åˆ†çš„æŠ½å–ã€åˆå¹¶ã€è½¬æ¢ã€‚æ”¯æŒå¸¸è§åè®®ï¼Œå¦‚ `file`, `imap`, `http(s)`, `rsync` ç­‰ç­‰ã€‚

#### `urlparse`

- `urlparse(urlstring, scheme='', allow_fragements=True)`
  - `urlstring` æ˜¯ä½ç½®å‚æ•°ï¼Œå¿…é¡»å¡«å†™ã€‚
  - `scheme` æŒ‡æ˜åè®®ã€‚é»˜è®¤ä¸º `http` æˆ–è€… `https` å¦‚æœé“¾æ¥æ²¡æœ‰æºå¸¦ `scheme` ä¿¡æ¯ã€‚è¿™ä¸ªé€‰é¡¹ä¸èƒ½è¦†ç›–ä¼ å…¥ã€€`url` ä¸­æŒ‡æ˜çš„åè®®ã€‚
  - `allow_fragments` æ˜¯å¦å¿½ç•¥ `fragment` å¹¶å°†å…¶è§£æä¸º `path/parameters/query` çš„ä¸€éƒ¨åˆ†ã€‚
  - å¯¹ `url` çš„è¯†åˆ«å’Œåˆ†æ®µã€‚è¿”å›ä¸€ä¸ª `urllib.parse.ParseResult` å¯¹è±¡ã€‚

- ```python
  from urllib.parse import urlparse
  
  res = urlparse("http://www.quizlet.com/index.html;user?id=15#comment")
  print(type(res), res)
  ```

  - è¿™ä¸ªå‘½åå…ƒç»„çš„æ„é€ ï¼š`ParseResult(scheme='http', netloc='www.quizlet.com', path='/index.html', params='user', query='id=15', fragment='comment')`
    - å¯ä»¥ä½¿ç”¨ç´¢å¼•é¡ºåºï¼Œç‚¹å±æ€§åçš„æ–¹å¼æ¥è·å–ã€‚
    - `://` ä¹‹å‰çš„éƒ¨åˆ†æ˜¯åè®®
    - `/` ä¹‹å‰çš„éƒ¨åˆ†æ˜¯ `netloc`
    - è®¿é—®è·¯å¾„ä¸º `path`
    - åˆ†å·å‰é¢ä¸º `params`
    - `?` åé¢æ˜¯ `query string` ä¸€èˆ¬åªåœ¨ GET è¯·æ±‚ä½¿ç”¨
    - `#` æ˜¯é”šç‚¹ï¼Œç›´æ¥å®šä½é¡µé¢å†…éƒ¨çš„ä½ç½®
  - https://skorks.com/2010/05/what-every-developer-should-know-about-urls/

- ä¸€ä¸ªæ ‡å‡†çš„ `url` çš„æ ¼å¼å¦‚ä¸‹ï¼š

  - `<scheme>://<username>:<password>@<host>:<port>/<path>;<parameters>?<query>#<fragment>`
  - ä¸Šè¿°çš„æ ¼å¼ä¸­ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰éƒ¨åˆ†éƒ½æ˜¯å¿…éœ€çš„ã€‚å¯ä»¥çœç•¥ã€‚

#### `urlunparse`

- æ¥å—ä¸€ä¸ªé•¿åº¦ä¸º 6 çš„å¯è¿­ä»£å¯¹è±¡ï¼Œç„¶åå°†å…¶è½¬ä¼šä¸ºä¸€ä¸ª `url` å­—ç¬¦ä¸²ã€‚é•¿åº¦å¿…é¡»æ˜¯ 6ã€‚

- è¿™ä¸ªå¯è¿­ä»£çš„å¯¹è±¡ï¼ŒæŒ‰ç…§ `scheme`, `netloc`, `path`, `params`, `query` å’Œ `fragment` çš„æ–¹å¼æä¾›å­—ç¬¦ä¸²ã€‚

  - ```python
    from urllib.parse import urlunparse
    
    data = ["http", "www.quizlet.com", "index.html", "user", "a=6&b=10", "comment"]
    print(urlunparse(data))
    ```

#### `urlsplit`/`urlunsplit`

- `urlunsplit` æ¥å—é•¿åº¦ä¸º `5` çš„å¯è¿­ä»£å¯¹è±¡ã€‚ä½†æ˜¯è¦æ±‚æŠŠ `params` åˆå¹¶åˆ° `path` ä¸­ã€‚
- `urlsplit` åˆ†å‰²ä¸€ä¸ª `url`ï¼Œè¿”å›å‘½åå…ƒç»„, `SplitResult` ä½œä¸ºç»“æœ

```python
from urllib.parse import urlsplit, urlunsplit

url = "https://user:passwd@www.quizlet.com:443/inner/index.html;param?id=10#spanish"
result = urlsplit(url)
print(
    result,  # SplitResult(scheme='https', netloc='user:passwd@www.quizlet.com:443', path='/inner/index.html;param', query='id=10', fragment='spanish')
    type(result),  # <class 'urllib.parse.SplitResult'>
    result.scheme,  # https
    result[0],  # https
    urlunsplit(result),  # Same as `url`
    sep="\n" + "-" * 8 + "\n",
)
```

#### `urljoin`

- æ¥å—ä¸€ä¸ªåŸºç¡€é“¾æ¥ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ï¼Œç„¶åæ–°çš„é“¾æ¥ä½œä¸ºç¬¬äºŒä¸ªå‚æ•°ã€‚è¯¥æ–¹æ³•è§£æç¬¬äºŒä¸ªé“¾æ¥çš„å†…å®¹ï¼ˆå¦‚åè®®ï¼Œè·¯å¾„ï¼Œç«¯å£ç­‰ï¼‰ï¼Œå¹¶å¯¹ç¬¬ä¸€ä¸ªé“¾æ¥çš„å†…å®¹è¿›è¡Œæ›¿æ¢/è¡¥å……ã€‚
- è¿”å›å­—ç¬¦ä¸²ä½œä¸ºæ‹¼æ¥çš„ç»“æœã€‚

```python
from urllib.parse import urljoin

print(
    urljoin(
        "http://www.google.com:80", "index.html"
    ),  # http://www.google.com:80/index.html
    urljoin(
        "http://www.google.com:80", "http://www.baidu.com/index.html"
    ),  # http://www.baidu.com/index.html
    urljoin(
        "http://www.google.com/about.html", "http://www.baidu.com/index.html"
    ),  # http://www.baidu.com/index.html
    urljoin(
        "http://www.google.com/about.html",
        "http://www.baidu.com/FAQ.html;user=10?question=2",
    ),  # http://www.baidu.com/FAQ.html;user=10?question=2
    urljoin(
        "http://www.google.com/about.html;user=10?question=10",
        "www.baidu.com/index.php",
    ),  # http://www.google.com/www.baidu.com/index.php
    urljoin(
        "http://www.google.com/about.html;user=10?question=10", "www.baidu.com"
    ),  # http://www.google.com/www.baidu.com
    urljoin(
        "www.google.com", "?category=2#comment"
    ),  # www.google.com?category=2#comment
    sep="\n",
)
```

- `base_url` ä¸­çš„ `scheme, netloc, path` ä¼šè¢«æ”¾åœ¨ä¸€èµ·ã€‚ä½†æ˜¯ `params, query, fragment` ç›´æ¥è¢«å¿½ç•¥ä¸è®¡ã€‚
- å¦‚æœä¸¤ä¸ª `url` æä¾›é‡å¤ä¿¡æ¯ï¼Œç¬¬äºŒä¸ª `url` çš„ä¿¡æ¯ä¼šè¦†ç›–ç¬¬ä¸€ä¸ª `url`ã€‚å¦‚æœç¬¬äºŒä¸ª  `url` ä¸èƒ½è¢«è§£æï¼Œä¸ä¼šæŠ¥é”™ï¼Œç›´æ¥è¿”å›ç¬¬äºŒä¸ªä½œä¸ºç»“æœã€‚

#### `dict` å’Œ `query/params` çš„è½¬åŒ–

- æŠŠå­—å…¸è½¬åŒ–ä¸ºå­—ç¬¦ä¸²ï¼ˆå‚æ•°è¡¨ç¤ºï¼‰ï¼Œç„¶åå†è½¬åŒ–ä¸º `bytes` å¯¹è±¡å°±å¯ä»¥ç”¨æ¥å‘æœåŠ¡å™¨å‘é€ä¸åŒçš„æ•°æ®äº†ã€‚

```python
from urllib.parse import parse_qs, parse_qsl, urlencode, urljoin

params = {"name": "germey", "age": 22}
base_url = "https://www.google.com"
url = urljoin(base_url, "?" + urlencode(params))
print(url)
```

- ååºåˆ—åŒ–é€šè¿‡ `parse_qs` çš„æ–¹å¼è½¬åŒ–ä¸ºå­—å…¸ï¼Œæˆ–è€… `parse_qsl` çš„æ–¹å¼è½¬åŒ–ä¸ºå…ƒç»„åˆ—è¡¨ã€‚

```python
query = urlencode(params)
print(parse_qs(query)) # {'name': ['germey'], 'age': ['22']}
print(parse_qsl(query)) # {'name': ['germey'], 'age': ['22']}
```

#### `quote`/`unquote`

- `quote` å°†å­—ç¬¦ä¸²è½¬åŒ–ä¸º `url` è½¬ä¹‰å­—ç¬¦çš„å½¢å¼ï¼Œ`unquote` å°†è½¬ä¹‰å­—ç¬¦å­—ç¬¦ä¸²è½¬åŒ–ä¸º `unicode` ç¼–ç å½¢å¼ã€‚

```python
from urllib.parse import quote, unquote, urlparse

kw = 'Linux ç³»ç»Ÿ'
url = f'https://www.google.com/s?wd={quote(kw)}'
print(url) # https://www.google.com/s?wd=Linux%20%E7%B3%BB%E7%BB%9F

quoted = urlparse(url).query
print(unquote(quoted)) # wd=Linux ç³»ç»Ÿ
```

### `robotparser`

#### `Robot` åè®®

- Robots åè®®ä¹Ÿæˆä¸ºçˆ¬è™«åè®®ã€æœºå™¨äººåè®®ï¼Œå®ƒçš„å…¨åæ˜¯ç½‘ç»œçˆ¬è™«æ’é™¤æ ‡å‡† (Robots Exclusion Protocol)
- è¿™ä¸ªåè®®å‘Šè¯‰çˆ¬è™«å’Œæœç´¢å¼•æ“å¯ä»¥æŠ“å–çš„éƒ¨åˆ†ï¼Œä»¥åŠä¸å¯ä»¥æŠ“å–çš„éƒ¨åˆ†ã€‚è¿™ä½œä¸º `robots.txt` çš„æ–‡ä»¶æ”¾ç½®åœ¨ç½‘ç«™çš„æ ¹ç›®å½•ä¸‹ã€‚
- çˆ¬è™«è®¿é—®ç«™ç‚¹æ—¶ï¼Œé¦–å…ˆæ£€æŸ¥æ ¹ç›®å½•ä¸‹æ˜¯å¦å­˜åœ¨æ­¤æ–‡ä»¶ã€‚å¦‚æœå­˜åœ¨ï¼Œçˆ¬è™«æ ¹æ®å…¶å®šä¹‰çš„çˆ¬å–ä¹å‘³çˆ¬å–ã€‚åä¹‹ï¼Œè®¿é—®æ‰€æœ‰å¯ä»¥ç›´æ¥è®¿é—®çš„é¡µé¢ã€‚

```txt
# æ‰€æœ‰çˆ¬è™«å—åˆ°è¿™ä¸ªé™åˆ¶
User-agent: *
Allow: /search
Disallow: /ajax
Disallow: /*/autosave$
# User-agent ä¸º WebCrawler çš„çˆ¬è™«ä¸å¯ä»¥çˆ¬å–ä»»ä½•å†…å®¹
User-agent: WebCrawler
Disallow: /
```

-  `User-agent` æè¿°çš„çˆ¬è™«çš„åç§°ã€‚
  - `*` ï¼šå¯¹äºæ‰€æœ‰çš„çˆ¬è™«æœ‰æ•ˆ
  - è®¾ç½®ä¸º `Baiduspider`ï¼Œé‚£ä¹ˆè¿™ä¸ªåè®®å°±ä»…å¯¹ç™¾åº¦çš„çˆ¬è™«æœ‰æ•ˆ
  - æŒ‡æ˜å¤šä¸ª `User-agent` å°±ä¼šæœ‰å¤šä¸ªçˆ¬è™«å—åˆ°é™åˆ¶
- `Disallow` æŒ‡å®šä¸å…è®¸æŠ“å–çš„ç›®å½•ã€‚
  - /` è¡¨ç¤ºç¦æ­¢æ‰€æœ‰é¡µé¢è¢«æŠ“å–ã€‚å¹¶ä¸”ï¼Œæ”¯æŒä½¿ç”¨ `*` ä½œä¸ºé€šé…ç¬¦ã€‚
  - ç•™ç©ºè¡¨ç¤ºå…è®¸ä»»ä½•ç›®å½•
- `Allow` å¯ä»¥è¦†ç›– `Disallow` çš„é™åˆ¶ï¼Œç”¨æ¥æ’é™¤æŸäº›éƒ¨åˆ†ã€‚ä¾‹å¦‚ `/search` è¡¨ç¤ºå¯ä»¥æŠ“å– search ç›®å½•
- ä¸Šè¿°è¿™ä¸ªæ–‡ä»¶å¯ä»¥ç•™ç©ºï¼Œè¡¨ç¤ºå…è®¸ä¸€åˆ‡ã€‚

#### å¸¸è§çˆ¬è™« UA å’Œç½‘ç«™

| çˆ¬è™« UA     | åç§°           |
| ----------- | -------------- |
| BaiduSpider | ç™¾åº¦           |
| Googlebot   | è°·æ­Œ           |
| 360Spider   | 360 æœç´¢       |
| YodaoBot    | æœ‰é“è¯å…¸       |
| ia_archiver | Alexa          |
| Scooter     | altavista/é›…è™ |

#### è§£æ `robots.txt`

- åˆ›å»ºä¸€ä¸ª `RobotFileParser` å¯¹è±¡

  - `RobotFileParser(url='')` ä¼ å…¥éœ€è¦è§£æçš„ç½‘é¡µ URL å³å¯ã€‚å¦‚æœç°åœ¨ä¸ä¼ å…¥ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ `set_url` è®¾ç½®ï¼Œé»˜è®¤ä¸ºç©ºã€‚

- è°ƒç”¨æ–¹æ³•

  - `set_url()` è®¾ç½®è¦è§£æçš„ URL
  - `read()` è¯»å–å¹¶åˆ†æã€‚ä¸ä¼šè‡ªåŠ¨è°ƒç”¨ï¼Œå¿…é¡»æ‰§è¡Œæ‰ä¼šè¯»å– `robots.txt` æ–‡ä»¶
  - `parse(lines: Iterable[str])` æ¥å—ä¸€ä¸ªå¯è¿­ä»£çš„å­—ç¬¦ä¸²ï¼Œä½œä¸º `robots.txt` çš„ä¸€éƒ¨åˆ†è¡Œï¼Œå¹¶å°†å…¶è¿›è¡Œè§£æã€‚
  - `can_fetch(User-agent, url)` åˆ¤æ–­æ˜¯å¦å¯ä»¥æŠ“å–è¿™ä¸ª `URL`ã€‚è¿”å› `True` æˆ–è€… `False`
  - `mtime()` è¿”å›ä¸Šä¼ æŠ“å–çš„åˆ†æçš„æ—¶é—´ã€‚å¦‚æœçˆ¬è™«è¿è¡Œæ—¶é—´é•¿ï¼Œè¿™å¯èƒ½æ˜¯æœ‰å¿…è¦çš„ã€‚
  - `modified()` æŠŠå½“å‰æ—¶é—´è®¾ç½®ä¸ºä¸Šæ¬¡æŠ“å–çš„åˆ†æçš„æ—¶é—´ã€‚

- ```python
  from urllib.robotparser import RobotFileParser
  
  r = RobotFileParser('https://www.google.com/robots.txt')
  r.read()
  print(r.can_fetch('*', 'https://www.google.com/m/finance'))
  print(r.can_fetch('*', 'https://www.google.com/search/about'))
  ```

- ```python
  from socket import timeout
  from urllib.error import HTTPError, URLError
  from urllib.request import Request, urlopen
  from urllib.robotparser import RobotFileParser
  
  r = RobotFileParser()
  
  headers = {
      "User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:100.0) Gecko/20100101 Firefox/100.0",
  }
  req = Request("https://quizlet.com/robots.txt", headers=headers, method="GET")
  try:
      content = urlopen(req).read().decode("utf-8")
  except HTTPError as e:
      print(e.reason, e.status, e.headers)
  except URLError as e:
      if isinstance(e.reason, timeout):
          print("Time out error")
      else:
          print(e.reason)
  r.parse(content.split("\n"))
  print(r.can_fetch(headers["User-Agent"], "https://quizlet.com/search "))
  ```

  - `quizlet` æ¯”è¾ƒç¥å¥‡ï¼Œå¦‚æœ `User-Agent` ä¸æ˜¯æ­£å¸¸çš„æµè§ˆå™¨çš„ `UA`ï¼Œç”šè‡³æ— æ³•è®¿é—® `robots.txt`ã€‚ä¸ºæ­¤ï¼Œæ‰€æœ‰çš„è°ƒç”¨éƒ½ä¼šè¿”å› `false`ï¼Œå’Œæ²¡æœ‰è°ƒç”¨ `read()` çš„ç»“æœä¸€æ ·ã€‚

## `requests`

- `requests` æä¾›äº†æ›´å¥½çš„ç½‘ç»œ `IO` è§£å†³æ–¹æ¡ˆ
  - å¯¹äº `HTTP` éªŒè¯å’Œ `Cookies`ï¼Œ`request` ä¸éœ€è¦æˆ‘ä»¬å†™ `Opener` å’Œ `Handler` æ¥å¤„ç†ã€‚

### `get`

- åŸºæœ¬ä¸Šå’Œ `urllib.request.urlopen(url, method='GET')` å·®ä¸å¤šã€‚

```python
import requests

r = requests.get("https://www.baidu.com")
print(
    type(r),  # <class 'requests.models.Response'>
    r.status_code,  # 200
    type(r.text),  # <class 'str'>
    r.text,  # html src
    type(r.cookies),  # <class 'requests.cookies.RequestsCookieJar'>
    r.cookies,  # <RequestsCookieJar[<Cookie BDORZ=27315 for .baidu.com/>]>
    type(r.content),  # bytes
    r.content,  # html src in bytes
    sep="\n",
)
```

- é€šè¿‡ä¼ªè£… `UA` æ¥çˆ¬å–çŸ¥ä¹çš„ç½‘é¡µã€‚å¦‚æœä¸åŠ å…¥è¿™ä¸ªï¼Œå¾€å¾€ä¼šå‡ºç° 403 forbidden ç­‰ï¼Œå› ä¸ºç½‘ç«™æ£€æµ‹åˆ°çˆ¬è™«æ­£åœ¨è®¿é—®è¿™ä¸ªç½‘é¡µã€‚

```python
import requests
import regex as re

headers = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:100.0) Gecko/20100101 Firefox/100.0"
}

r = requests.get("https://www.zhihu.com/explore", headers=headers)

# An failed attempt to use regex to parse something out of it.
'''
(?(DEFINE)
(?'selfcloseelement'(area|base|br|col|command|embed|hr|img|input|keygen|link|meta|param|source|track|wbr))
(?'selfclose'<(?P>selfcloseelement)[^<>]+>|<[^<>]+\/>)
(?'middle'[^<>]+|(?P>selfclose)))
<div\s+class="ExploreHomePage-square">
    (?P<pair>
    <(?<element>(?!(?P>selfcloseelement))\w+)[^<>]*>
        (?:(?P>middle)*|(?P>pair)*)
    <\/\g{element}>)*
<\/div>
'''
```

- æŠ“å–ï¼Œä¸‹è½½äºŒè¿›åˆ¶æ•°æ®

```python
import requests

r = requests.get("https://www.github.com/favicon.ico")
# print(r.text)
# print(r.content)
with open("github.ico", "w+b") as f:
    f.write(r.content)
```

- æ˜¾è€Œæ˜“è§çš„ï¼Œ`text` ç»™å‡ºäº†ä¸€å †ä¹±ç ã€‚è€Œ `content` ç»™å‡ºäº† bytes ç±»å‹çš„äºŒè¿›åˆ¶æ•°æ®ã€‚
- é€šè¿‡å†™å…¥ `wb` å°±å¯ä»¥å°†å…¶ä¿å­˜åˆ°æ–‡ä»¶ä¸­ã€‚

### `post`

- ç›´æ¥å°†å­—å…¸ç±»å‹çš„ `data` è½¬å…¥åˆ° `post` æ–¹æ³•ä¸­å³å¯ã€‚

```python
import requests

data = {"name": "yubo", "age": 16}
r = requests.post("https://httpbin.org/post", data=data)
print(r.text)
```

### `response` å¯¹è±¡çš„å±æ€§

- `status_code` çŠ¶æ€ç ï¼Œ`int`
- `headers` å“åº”å¤´ï¼Œ`requests.structures.CaseInsensitiveDict`
- `cookies` Cookiesï¼Œ`requests.cookies.RequestCookieJar`
- `url` urlï¼Œ`str`
- `history` è¯·æ±‚å†å²ï¼Œ`List`

```python
import requests

r = requests.get("https://www.jianshu.com")
attrs = ["status_code", "headers", "cookies", "url", "history"]
for t, attr in zip(
    map(lambda attr: repr(type(getattr(r, attr))), attrs),
    map(lambda attr: getattr(r, attr), attrs),
):
    print(t.ljust(40), attr)
```

```txt
<class 'int'>                            403
<class 'requests.structures.CaseInsensitiveDict'> {'Server': 'Tengine', 'Content-Type': 'text/html', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Date': 'Sat, 28 May 2022 21:06:00 GMT', 'Vary': 'Accept-Encoding', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'Content-Encoding': 'gzip', 'x-alicdn-da-ups-status': 'endOs,0,403', 'Via': 'cache27.l2cm12-6[7,0], cache6.us16[200,0]', 'Timing-Allow-Origin': '*', 'EagleId': '0819529a16537719608501554e'}
<class 'requests.cookies.RequestsCookieJar'> <RequestsCookieJar[]>
<class 'str'>                            https://www.jianshu.com/
<class 'list'>                           []
```

#### çŠ¶æ€ç 

- `requests.codes` æ˜¯ä¸€ä¸ªæšä¸¾ç±»ï¼Œå¯ä»¥ç”¨æ¥å¯¹çŠ¶æ€ç è¿›è¡Œåˆ¤æ–­ã€‚

- ```python
  import requests
  
  r = requests.get("https://www.google.com")
  if r.status_code == requests.codes.ok:
      print("Ok")
  else:
      print("Failed")
  ```

- æ›´å¤šå¯è§ [[Internet Basic, HTTP(s), TCP, and UDP]]ã€‚å¸¸è§çš„çŠ¶æ€ç åœ¨è¿™é‡Œéƒ½å·²å­—å…¸å½¢å¼æä¾›äº†ã€‚

### é«˜çº§ç”¨æ³•

#### ä¸Šä¼ æ–‡ä»¶

```python
import requests

files = {"file": open("github.ico", "rb")}
r = requests.post("https://httpbin.org/post", files=files)
print(r.text)
```

- ç½‘ç«™è¿”å›çš„æ—¶å€™åŒ…å«ä¸€ä¸ª `files` å­—æ®µã€‚è¯¥å­—æ®µä¸­è®°å½•äº†ä¸Šä¼ çš„æ–‡ä»¶ã€‚
- éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ`form` å­—æ®µæ˜¯ç©ºçš„ã€‚

#### Cookies

##### è·å–

```python
import requests

r = requests.get("https://www.google.com")
print(type(r), r.cookies)

[print(f"{k:<10}={v}") for k, v in r.cookies.items()]
```

- éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ`RequestCookieJar` æ²¡æœ‰ä¿å­˜çš„é€‰é¡¹ã€‚ä¸è¿‡ï¼Œéšä¾¿ä½¿ç”¨ `json` ä¿å­˜ä¸€ä¸‹ä¹Ÿè¶³å¤Ÿäº†ã€‚

##### ä¼ å…¥

- ä»¥å­¦æ ¡çš„ç½‘é¡µä¸ºä¾‹ï¼Œé¦–å…ˆç™»å½•å¹¶ä¸”è·å¾— `Cookie` çš„å†…å®¹ï¼ˆæ‰“å¼€ç½‘é¡µï¼Œç„¶åå¤åˆ¶ `Cookies: ` ä¹‹åçš„è¯·æ±‚å¤´çš„å†…å®¹ï¼‰

- é€šè¿‡ç›´æ¥å°† `cookies` ä½œä¸º `header` çš„ä¸€éƒ¨åˆ†ï¼Œå¯ä»¥ä¼ å…¥ `cookies`

```python
from email import header
import requests

cookie = "ai_user=XlC7FG14rMQkeifu8Z7zGy|2022-05-28T11:36:00.441Z; PVUE=08; JSESSIONID=0000sIfLWpzKUVmyyqDpjQ5-TRy:1cgcq4nr3; trdipcktrffcext=1"

headers = {
    "Cookie": cookie,
    "Host": "apps.gwinnett.k12.ga.us",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.64 Safari/537.36 Edg/101.0.1210.53",
}

r = requests.get(
    "https://apps.gwinnett.k12.ga.us/dca/student/dashboard", headers=headers
)
print(r.text)
```

- ä¹Ÿå¯ä»¥é€šè¿‡æ„å»º `RequestCookieJar` æˆ–è€…ç›´æ¥ä¼ å…¥å­—å…¸çš„æ–¹å¼æ¥ä¼ å…¥ `cookie`ã€‚ä»£ç ä¸­å±•ç¤ºçš„æ–¹å¼ï¼Œä»»æ„ä¸€ä¸ªéƒ½å¯ä»¥ç”¨æ¥å®ç°å¯¹ `cookie` çš„ç®¡ç†ã€‚

```python
import requests
from requests import cookies

cookies_str = "ai_user=XlC7FG14rMQkeifu8Z7zGy|2022-05-28T11:36:00.441Z; PD-H-SESSION-ID=1_OzaWfIiFIEqhAXIX2zYDL4ftOat8Z10EGLtJN9aaMsBJVEGHhOM=_AAAAAAA=_anGTbqr8zIwAhkILKFPTiB5wqEE=; PD_STATEFUL_1f058092-232b-11ec-bfa4-001a4a16017e=%2Fspvue; ASP.NET_SessionId=axgiishraj2xnvbbqqe5slev; PVUE=08; JSESSIONID=0000sIfLWpzKUVmyyqDpjQ5-TRy:1cgcq4nr3; trdipcktrffcext=1"

# Pass dict directly is accepted
jar = {k: v for k, v in (cookie.split("=", 1) for cookie in cookies_str.split(";"))}

# RequestCookieJar.set
jar = cookies.RequestsCookieJar()
for cookie in cookies_str.split(";"):
    k, v = cookie.split("=", 1)
    jar.set(k, v)

# Wrap in cookie jar
jar = cookies.cookiejar_from_dict(jar)

headers = {
    "Host": "apps.gwinnett.k12.ga.us",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.64 Safari/537.36 Edg/101.0.1210.53",
}

r = requests.get(
    "https://apps.gwinnett.k12.ga.us/dca/student/dashboard",
    cookies=jar,
    headers=headers,
)

print(r.text)
```

#### ä¼šè¯

- æˆ‘ä»¬ä½¿ç”¨ `get` å’Œ `post` æ–¹æ³•ï¼Œå®é™…ä¸Šæ˜¯ä½¿ç”¨å¤šä¸ªæµè§ˆå™¨æ‰“å¼€ä¸åŒçš„é¡µé¢ã€‚è¿™è¦æ±‚æˆ‘ä»¬æ‰‹åŠ¨çš„åœ¨å¤šæ¬¡è°ƒç”¨ä¹‹é—´åŒæ­¥ cookiesï¼Œæ¯”è¾ƒç¹çã€‚
- ä¸ºäº†å‡è½»è¿™ä¹ˆåšçš„è´Ÿæ‹…ï¼Œæˆ‘ä»¬å¯ä»¥ç»´æŒä¸€ä¸ªä¼šè¯ï¼Œä¸ºæ­¤ï¼Œä¸ç”¨æ‹…å¿ƒ `cookies` çš„é—®é¢˜ã€‚

```python
import requests

with requests.Session() as s:
    s.get("https://httpbin.org/cookies/set/number/123456789")
    r = s.get("https://httpbin.org/cookies")
    print(r.text)
```

- æˆ‘ä»¬å‘ç°ï¼Œ`cookies` åœ¨ç¬¬ä¸€æ¬¡è¯·æ±‚æœåŠ¡å™¨æ—¶å¾—åˆ°çš„ï¼Œä¿ç•™äº†ä¸‹æ¥ã€‚

#### SSL

- å¦‚æœä¸€ä¸ªç½‘ç«™çš„ `SSL` è¯ä¹¦æœ‰é—®é¢˜ï¼Œæˆ–è€…æ˜¯è‡ªç­¾å‘çš„ï¼Œåœ¨ä½¿ç”¨ `get` æ–¹æ³•æ—¶å°±ä¼šå‡ºç° `SSLError`
- ä¸ºäº†é¿å…è¿™ä¸ªé—®é¢˜ï¼Œåœ¨ `get` æ–¹æ³•ä¼ å…¥ `verify=False` å³å¯ã€‚è¿™ä¼šé€ æˆè­¦å‘Š
  - é€šè¿‡ `urllib3.disable_warnings()` çš„æ–¹å¼æ¥å…³é—­è­¦å‘Š
  - `logging.captureWarning(True)` æ•è·è­¦å‘Šåˆ°æ—¥å¿—æ¥å¿½ç•¥
  - æˆ–è€…æŒ‡å®šæœ¬åœ°è¯ä¹¦ä½œä¸ºå®¢æˆ·ç«¯è¯ä¹¦ã€‚å¯ä»¥æ˜¯å•ä¸ªæ–‡ä»¶ï¼ŒåŒ…å«å¯†é’¥å’Œè¯ä¹¦ï¼›ä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªï¼ˆè¯ä¹¦è·¯å¾„ï¼Œå¯†é’¥è·¯å¾„ï¼‰çš„å…ƒç»„ã€‚è¿™è¦æ±‚è·å¾— `crt` å’Œ `key` æ–‡ä»¶ï¼Œå¹¶ä¸”ä»–ä»¬å¿…é¡»æ˜¯è§£å¯†çš„

```python
import requests
import urllib3
import logging

logging.captureWarnings(True)
urllib3.disable_warnings()

response = requests.get('https://www.12306.cn', verify=False) # cert=('path/server.crt', 'path/key')
print(response.status_code)
```

#### ä»£ç†

- å¯ä»¥ç›´æ¥å‘ `requests.get` æ–¹æ³•ä¼ å…¥ä»£ç†

```python
import requests

proxies = {
    "https":"http://10.10.1.10:1080",
    "http": "http://10.10.1.10:3128"
}
requests.get("https://www.taobao.com", proxies=proxies)
```

- é€šè¿‡å®‰è£… `requests[socks]`ï¼Œè¿˜å¯ä»¥æ”¯æŒä½¿ç”¨ `socks` åè®®ä»£ç†ã€‚

#### éªŒè¯

- å¯ä»¥ç›´æ¥ä¼ å…¥ `auth=(user, passwd)` çš„æ–¹å¼æ¥è¿›è¡ŒéªŒè¯
  - ä¹Ÿå¯ä»¥ä¼ å…¥ä¸€ä¸ª `HTTPBasicAuth` ç±»å¯¹è±¡æ¥è¿›è¡ŒéªŒè¯
- å¯ä»¥é€šè¿‡ `https://user:passwd@example.com` çš„æ–¹å¼è¿›è¡ŒéªŒè¯

```python
import requests
from requests.auth import HTTPBasicAuth

r = requests.get('https://www.google.com', auth=HTTPBasicAuth('user', 'passwd'))
# Or, requests.get('https://www.google.com', auth=('user', 'passwd'))
print(r.status_code)
```

- ç”šè‡³å¯ä»¥ä½¿ç”¨ `OAuth` ã€‚é€šè¿‡å®‰è£… `requests_oauthlib` è¿›è¡Œã€‚è§ https://github.com/requests/requests-oauthlib

#### è¶…æ—¶

- è®¾ç½® `timeout` å‚æ•°æ¥è¿›è¡Œè¶…æ—¶è®¾ç½®
  - å¯ä»¥æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°ï¼Œä¹Ÿå°±æ˜¯è¿æ¥æœåŠ¡å™¨å’Œè¯»å–å†…å®¹æ—¶é—´çš„æ€»å’Œä¸è¶…è¿‡è¿™ä¸ªæ—¶é—´
  - å¯ä»¥æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œï¼ˆè¿æ¥æ—¶é—´ï¼Œè¯»å–æ—¶é—´ï¼‰
  - å¦‚æœå¸Œæœ›æ°¸ä¹…ç­‰å¾…ï¼Œå¯ä»¥ä¸è®¾ç½®æˆ–è€…è®¾ç½®ä¸º `None`ã€‚è¿™æ ·ï¼Œæ°¸è¿œä¸ä¼šå‘ç”Ÿè¶…æ—¶é”™è¯¯ã€‚

#### Prepared Request

```python
from requests import Request, Session

url = "http://httpbin.org/post"
data = {"name": "yubo", "age": 16}
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.64 Safari/537.36 Edg/101.0.1210.53"
}

with Session() as s:
    req = Request("POST", url, data=data, headers=headers)
    prep = s.prepare_request(req)
    r = s.send(prep)
    print(r.text)
```
